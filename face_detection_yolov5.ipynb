{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czn2fhoKlZ28","executionInfo":{"status":"ok","timestamp":1678178991239,"user_tz":-60,"elapsed":25859,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"f7f4e8aa-c64f-4d9f-9dbe-16b79b7ecdd7"},"id":"czn2fhoKlZ28","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"id":"a2f63819","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2f63819","executionInfo":{"status":"ok","timestamp":1678179037782,"user_tz":-60,"elapsed":12161,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"cf38dae7-360e-4a9c-f3f5-0233ade954c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (0.11.2)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.10.1)\n","Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.8/dist-packages (from seaborn) (3.5.3)\n","Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.3.5)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.22.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (8.4.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (23.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (4.38.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23->seaborn) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (6.0)\n"]}],"source":["#install the required packages\n","!pip install torch torchvision\n","!pip install seaborn\n","!pip install -U PyYAML\n","from IPython.display import Image #this is to render predictions"]},{"cell_type":"code","execution_count":4,"id":"5374cc02","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5374cc02","executionInfo":{"status":"ok","timestamp":1678179105063,"user_tz":-60,"elapsed":2067,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"29b5f8e1-4172-44b7-d9b8-6776ebdabd3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 15291, done.\u001b[K\n","remote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 15291 (delta 0), reused 1 (delta 0), pack-reused 15287\u001b[K\n","Receiving objects: 100% (15291/15291), 14.19 MiB | 28.84 MiB/s, done.\n","Resolving deltas: 100% (10483/10483), done.\n"]}],"source":["#clone YOLOv5 model to use for face detection\n","!git clone https://github.com/ultralytics/yolov5.git"]},{"cell_type":"code","source":["%cd yolov5/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufE15YtLmHWU","executionInfo":{"status":"ok","timestamp":1678179123946,"user_tz":-60,"elapsed":234,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"f7cf4dda-7632-44b9-95f4-eaa5802bd62c"},"id":"ufE15YtLmHWU","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov5\n"]}]},{"cell_type":"code","execution_count":6,"id":"a98608b0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a98608b0","executionInfo":{"status":"ok","timestamp":1678179136171,"user_tz":-60,"elapsed":9490,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"54613d02-30fb-4cdf-ff65-984dbd7e6eb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gitpython>=3.1.30\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (3.5.3)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.22.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (4.6.0.66)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (8.4.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (5.4.8)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (6.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (2.25.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (1.10.1)\n","Collecting thop>=0.1.1\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (1.13.1+cu116)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (0.14.1+cu116)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 17)) (4.64.1)\n","Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 21)) (2.11.2)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 26)) (1.3.5)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 27)) (0.11.2)\n","Collecting setuptools>=65.5.1\n","  Downloading setuptools-67.5.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (4.38.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (3.0.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (23.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (1.4.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (1.26.14)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->-r requirements.txt (line 15)) (4.5.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.4.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.4.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.2.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.8.1)\n","Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.19.6)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.51.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.16.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.38.4)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 26)) (2022.7.1)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (4.9)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 21)) (6.0.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.2.2)\n","Installing collected packages: smmap, setuptools, thop, gitdb, gitpython\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\n","cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.5.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gitdb-4.0.10 gitpython-3.1.31 setuptools-67.5.1 smmap-5.0.0 thop-0.1.1.post2209072238\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":7,"id":"3e0ccaa7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3e0ccaa7","executionInfo":{"status":"ok","timestamp":1678180646918,"user_tz":-60,"elapsed":1447805,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"5283434f-f961-488a-d669-4b12e7829d3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/drive/MyDrive/FaceDetectionDataset/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=30, batch_size=16, imgsz=720, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=/content/drive/MyDrive/your_results_folder_name, name=my_yolo, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n","YOLOv5 ğŸš€ v7.0-117-g85f6019 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ğŸš€ in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ğŸš€ runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/drive/MyDrive/your_results_folder_name', view at http://localhost:6006/\n","2023-03-07 08:53:25.111706: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-07 08:53:26.045772: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n","2023-03-07 08:53:26.045899: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n","2023-03-07 08:53:26.045919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 16.3MB/s]\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n","100% 14.1M/14.1M [00:00<00:00, 124MB/s] \n","\n","Overriding model.yaml nc=80 with nc=4\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model summary: 214 layers, 7030417 parameters, 7030417 gradients, 16.0 GFLOPs\n","\n","Transferred 343/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","WARNING âš ï¸ --img-size 720 must be multiple of max stride 32, updating to 736\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/FaceDetectionDataset/train/labels.cache... 784 images, 1 backgrounds, 0 corrupt: 100% 785/785 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/FaceDetectionDataset/valid/labels.cache... 336 images, 0 backgrounds, 0 corrupt: 100% 336/336 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.64 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n","Plotting labels to /content/drive/MyDrive/your_results_folder_name/my_yolo/labels.jpg... \n","Image sizes 736 train, 736 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1m/content/drive/MyDrive/your_results_folder_name/my_yolo\u001b[0m\n","Starting training for 30 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       0/29      4.93G     0.1209      0.036    0.05346         40        736:   0% 0/50 [00:01<?, ?it/s]WARNING âš ï¸ TensorBoard graph visualization failure Sizes of tensors must match except in dimension 1. Expected size 46 but got size 45 for tensor number 1 in the list.\n","       0/29      4.98G    0.08309    0.03069    0.04045          2        736: 100% 50/50 [00:42<00:00,  1.18it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:42<00:00,  3.90s/it]\n","                   all        336        361      0.019      0.848     0.0795     0.0236\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       1/29      6.16G     0.0513    0.02283    0.03379          1        736: 100% 50/50 [00:38<00:00,  1.31it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:06<00:00,  1.72it/s]\n","                   all        336        361       0.18      0.376       0.16     0.0638\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       2/29      6.16G    0.05051    0.01839    0.02985          1        736: 100% 50/50 [00:36<00:00,  1.36it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.49it/s]\n","                   all        336        361      0.171      0.462      0.204     0.0728\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       3/29      6.16G    0.04436    0.01663    0.02405          0        736: 100% 50/50 [00:35<00:00,  1.40it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:09<00:00,  1.16it/s]\n","                   all        336        361      0.241      0.225      0.168     0.0747\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       4/29      6.16G    0.04217    0.01643    0.02418          2        736: 100% 50/50 [00:35<00:00,  1.41it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:06<00:00,  1.63it/s]\n","                   all        336        361      0.289      0.156      0.166     0.0677\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       5/29      6.16G    0.03885    0.01547    0.02178          4        736: 100% 50/50 [00:36<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.49it/s]\n","                   all        336        361      0.527      0.465       0.38      0.176\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       6/29      6.16G    0.03777     0.0154    0.02012          2        736: 100% 50/50 [00:35<00:00,  1.40it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.55it/s]\n","                   all        336        361       0.43      0.563      0.487      0.225\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       7/29      6.16G    0.03878    0.01511    0.01781          4        736: 100% 50/50 [00:35<00:00,  1.39it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.45it/s]\n","                   all        336        361      0.655      0.631      0.705      0.387\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       8/29      6.16G    0.03657     0.0145    0.01536          1        736: 100% 50/50 [00:35<00:00,  1.41it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.46it/s]\n","                   all        336        361      0.547      0.548       0.59      0.328\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       9/29      6.16G    0.03645    0.01312    0.01302          2        736: 100% 50/50 [00:37<00:00,  1.32it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.49it/s]\n","                   all        336        361      0.754      0.774      0.813      0.448\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      10/29      6.16G    0.03338    0.01377    0.01231          3        736: 100% 50/50 [00:35<00:00,  1.42it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.38it/s]\n","                   all        336        361      0.724      0.691      0.747      0.411\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      11/29      6.16G    0.03422    0.01344    0.01234          2        736: 100% 50/50 [00:35<00:00,  1.41it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.39it/s]\n","                   all        336        361      0.714      0.771      0.771      0.457\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      12/29      6.16G    0.03441    0.01338    0.01301          3        736: 100% 50/50 [00:35<00:00,  1.40it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.45it/s]\n","                   all        336        361      0.755      0.794      0.855      0.532\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      13/29      6.16G    0.03356    0.01344    0.01158          4        736: 100% 50/50 [00:35<00:00,  1.39it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.47it/s]\n","                   all        336        361      0.675      0.738      0.791      0.484\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      14/29      6.16G    0.02868    0.01259   0.009906          1        736: 100% 50/50 [00:37<00:00,  1.33it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:06<00:00,  1.69it/s]\n","                   all        336        361      0.795      0.787      0.873      0.523\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      15/29      6.16G    0.03018    0.01275   0.009599          2        736: 100% 50/50 [00:36<00:00,  1.37it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:06<00:00,  1.71it/s]\n","                   all        336        361      0.725      0.792       0.83      0.516\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      16/29      6.16G    0.02782      0.012   0.008714          1        736: 100% 50/50 [00:35<00:00,  1.39it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:06<00:00,  1.79it/s]\n","                   all        336        361      0.864      0.827      0.927      0.617\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      17/29      6.16G    0.02956    0.01232   0.008461          2        736: 100% 50/50 [00:36<00:00,  1.36it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:06<00:00,  1.78it/s]\n","                   all        336        361      0.894      0.865      0.942      0.631\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      18/29      6.16G    0.03285     0.0118   0.008806          2        736: 100% 50/50 [00:37<00:00,  1.34it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:06<00:00,  1.62it/s]\n","                   all        336        361      0.888      0.907      0.939      0.676\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      19/29      6.16G    0.02865    0.01159   0.007112          1        736: 100% 50/50 [00:36<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.55it/s]\n","                   all        336        361      0.873      0.903      0.939      0.644\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      20/29      6.16G    0.02874    0.01222   0.008259          2        736: 100% 50/50 [00:38<00:00,  1.29it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.45it/s]\n","                   all        336        361       0.86      0.894      0.922      0.613\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      21/29      6.16G    0.02411    0.01141   0.007228          3        736: 100% 50/50 [00:35<00:00,  1.40it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.43it/s]\n","                   all        336        361      0.902      0.884      0.943      0.672\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      22/29      6.16G    0.02529    0.01164   0.007331          3        736: 100% 50/50 [00:35<00:00,  1.40it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.45it/s]\n","                   all        336        361       0.93      0.904      0.957      0.704\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      23/29      6.16G    0.02588    0.01202    0.00622          7        736: 100% 50/50 [00:35<00:00,  1.41it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.41it/s]\n","                   all        336        361      0.864      0.918      0.953      0.697\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      24/29      6.16G    0.02458    0.01129    0.00678          3        736: 100% 50/50 [00:35<00:00,  1.40it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.49it/s]\n","                   all        336        361      0.808      0.865      0.902      0.638\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      25/29      6.16G    0.02357     0.0112   0.006904          1        736: 100% 50/50 [00:37<00:00,  1.34it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.43it/s]\n","                   all        336        361      0.915      0.914      0.966      0.715\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      26/29      6.16G    0.02386    0.01122   0.005369          1        736: 100% 50/50 [00:35<00:00,  1.41it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.44it/s]\n","                   all        336        361      0.882      0.952      0.965       0.73\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      27/29      6.16G    0.02096    0.01026   0.004194          1        736: 100% 50/50 [00:35<00:00,  1.40it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.45it/s]\n","                   all        336        361      0.914      0.928       0.97      0.726\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      28/29      6.16G    0.02296    0.01021   0.004655          4        736: 100% 50/50 [00:36<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.38it/s]\n","                   all        336        361      0.933      0.921      0.977      0.764\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      29/29      6.16G    0.02064   0.009779   0.003966          2        736: 100% 50/50 [00:36<00:00,  1.37it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:07<00:00,  1.42it/s]\n","                   all        336        361      0.919      0.968      0.982      0.772\n","\n","30 epochs completed in 0.385 hours.\n","Optimizer stripped from /content/drive/MyDrive/your_results_folder_name/my_yolo/weights/last.pt, 14.5MB\n","Optimizer stripped from /content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt, 14.5MB\n","\n","Validating /content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt...\n","Fusing layers... \n","Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:08<00:00,  1.31it/s]\n","                   all        336        361      0.916      0.968      0.982      0.771\n","                device        336        105       0.88       0.99      0.992      0.826\n","                  live        336         80      0.876      0.963      0.974      0.651\n","                  mask        336         79       0.98      0.987      0.994      0.835\n","                 photo        336         97      0.928      0.934      0.969      0.772\n","Results saved to \u001b[1m/content/drive/MyDrive/your_results_folder_name/my_yolo\u001b[0m\n"]}],"source":["!python train.py --img 720 --batch 16 --epochs 30 --data /content/drive/MyDrive/FaceDetectionDataset/data.yaml --weights yolov5s.pt  --name my_yolo --project /content/drive/MyDrive/your_results_folder_name"]},{"cell_type":"code","execution_count":8,"id":"bef14f3d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bef14f3d","executionInfo":{"status":"ok","timestamp":1678180957577,"user_tz":-60,"elapsed":10811,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"1a2962f1-7222-41b0-928c-1841f5724789"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt'], source=/content/drive/MyDrive/FaceDetectionDataset/test/images/113255_jpg.rf.ce97fd42e31242f3c3e35beae2ae88b7.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/content/drive/MyDrive/your_results_folder_name/my_yolo, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 ğŸš€ v7.0-117-g85f6019 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n","image 1/1 /content/drive/MyDrive/FaceDetectionDataset/test/images/113255_jpg.rf.ce97fd42e31242f3c3e35beae2ae88b7.jpg: 640x640 1 live, 12.8ms\n","Speed: 0.7ms pre-process, 12.8ms inference, 1.7ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1m/content/drive/MyDrive/your_results_folder_name/my_yolo/exp\u001b[0m\n"]}],"source":["!python detect.py  --source /content/drive/MyDrive/FaceDetectionDataset/test/images/113255_jpg.rf.ce97fd42e31242f3c3e35beae2ae88b7.jpg --weights /content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt --project /content/drive/MyDrive/your_results_folder_name/my_yolo"]},{"cell_type":"code","source":["# validating the model performance before pruning\n","!python val.py --weights /content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt  --data /content/drive/MyDrive/FaceDetectionDataset/data.yaml --img 720 --iou 0.65 --project /content/drive/MyDrive/your_results_folder_name/my_yolo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YT1XnAGWy1wh","executionInfo":{"status":"ok","timestamp":1678181009227,"user_tz":-60,"elapsed":23228,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"bf160cca-acd7-4b15-8ff1-7ec57ec0ed56"},"id":"YT1XnAGWy1wh","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=/content/drive/MyDrive/FaceDetectionDataset/data.yaml, weights=['/content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt'], batch_size=32, imgsz=720, conf_thres=0.001, iou_thres=0.65, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=/content/drive/MyDrive/your_results_folder_name/my_yolo, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 ğŸš€ v7.0-117-g85f6019 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n","WARNING âš ï¸ --img-size 720 must be multiple of max stride 32, updating to 736\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/FaceDetectionDataset/valid/labels.cache... 336 images, 0 backgrounds, 0 corrupt: 100% 336/336 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:10<00:00,  1.08it/s]\n","                   all        336        361      0.931      0.947      0.981      0.772\n","                device        336        105      0.897      0.981      0.992      0.826\n","                  live        336         80      0.913       0.95      0.973       0.65\n","                  mask        336         79      0.981      0.987      0.993      0.836\n","                 photo        336         97      0.934      0.869      0.967      0.775\n","Speed: 0.6ms pre-process, 8.4ms inference, 2.6ms NMS per image at shape (32, 3, 736, 736)\n","Results saved to \u001b[1m/content/drive/MyDrive/your_results_folder_name/my_yolo/exp2\u001b[0m\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"DpNAkSLA3CG5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678181743496,"user_tz":-60,"elapsed":22581,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"6357ab6e-20aa-463b-d864-a5d91c1c9e58"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=/content/drive/MyDrive/FaceDetectionDataset/data.yaml, weights=['/content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt'], batch_size=32, imgsz=720, conf_thres=0.001, iou_thres=0.65, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=/content/drive/MyDrive/your_results_folder_name/my_yolo, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 ğŸš€ v7.0-117-g85f6019 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n","WARNING âš ï¸ --img-size 720 must be multiple of max stride 32, updating to 736\n","Model pruned to 0.0999 global sparsity\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/FaceDetectionDataset/valid/labels.cache... 336 images, 0 backgrounds, 0 corrupt: 100% 336/336 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:11<00:00,  1.01s/it]\n","                   all        336        361      0.919      0.961      0.981      0.762\n","                device        336        105      0.882      0.981      0.991      0.824\n","                  live        336         80      0.878      0.963      0.972       0.63\n","                  mask        336         79       0.98      0.987      0.993      0.823\n","                 photo        336         97      0.937      0.913      0.968       0.77\n","Speed: 4.3ms pre-process, 8.4ms inference, 2.4ms NMS per image at shape (32, 3, 736, 736)\n","Results saved to \u001b[1m/content/drive/MyDrive/your_results_folder_name/my_yolo/exp3\u001b[0m\n"]}],"source":["## Pruning 10%\n","!python val.py --weights /content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt  --data /content/drive/MyDrive/FaceDetectionDataset/data.yaml --img 720 --iou 0.65 --project /content/drive/MyDrive/your_results_folder_name/my_yolo"],"id":"DpNAkSLA3CG5"},{"cell_type":"code","source":["## pruning 20%\n","!python val.py --weights /content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt  --data /content/drive/MyDrive/FaceDetectionDataset/data.yaml --img 720 --iou 0.65 --project /content/drive/MyDrive/your_results_folder_name/my_yolo"],"metadata":{"id":"sv_-7rJcy2jo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678182216299,"user_tz":-60,"elapsed":22711,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"4925328c-b883-4136-bdda-0ab1903c5f89"},"id":"sv_-7rJcy2jo","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=/content/drive/MyDrive/FaceDetectionDataset/data.yaml, weights=['/content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt'], batch_size=32, imgsz=720, conf_thres=0.001, iou_thres=0.65, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=/content/drive/MyDrive/your_results_folder_name/my_yolo, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 ğŸš€ v7.0-117-g85f6019 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n","WARNING âš ï¸ --img-size 720 must be multiple of max stride 32, updating to 736\n","Model pruned to 0.2 global sparsity\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/FaceDetectionDataset/valid/labels.cache... 336 images, 0 backgrounds, 0 corrupt: 100% 336/336 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:11<00:00,  1.01s/it]\n","                   all        336        361      0.938      0.945      0.975      0.741\n","                device        336        105      0.893      0.981      0.989      0.799\n","                  live        336         80       0.95      0.912      0.952      0.596\n","                  mask        336         79      0.985      0.987      0.994       0.81\n","                 photo        336         97      0.926      0.898      0.965       0.76\n","Speed: 0.7ms pre-process, 8.1ms inference, 2.9ms NMS per image at shape (32, 3, 736, 736)\n","Results saved to \u001b[1m/content/drive/MyDrive/your_results_folder_name/my_yolo/exp4\u001b[0m\n"]}]},{"cell_type":"code","source":["## pruning 30%\n","!python val.py --weights /content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt  --data /content/drive/MyDrive/FaceDetectionDataset/data.yaml --img 720 --iou 0.65 --project /content/drive/MyDrive/your_results_folder_name/my_yolo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xgZGMANedOrT","executionInfo":{"status":"ok","timestamp":1678182445149,"user_tz":-60,"elapsed":22481,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"8a4d237f-02e8-4d96-ec64-f4686bc7e8ce"},"id":"xgZGMANedOrT","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=/content/drive/MyDrive/FaceDetectionDataset/data.yaml, weights=['/content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt'], batch_size=32, imgsz=720, conf_thres=0.001, iou_thres=0.65, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=/content/drive/MyDrive/your_results_folder_name/my_yolo, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 ğŸš€ v7.0-117-g85f6019 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n","WARNING âš ï¸ --img-size 720 must be multiple of max stride 32, updating to 736\n","Model pruned to 0.3 global sparsity\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/FaceDetectionDataset/valid/labels.cache... 336 images, 0 backgrounds, 0 corrupt: 100% 336/336 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:11<00:00,  1.04s/it]\n","                   all        336        361      0.917      0.917      0.958      0.678\n","                device        336        105       0.88      0.971      0.989      0.748\n","                  live        336         80      0.929      0.823      0.901      0.509\n","                  mask        336         79          1      0.946      0.994      0.776\n","                 photo        336         97      0.861      0.928       0.95      0.682\n","Speed: 1.3ms pre-process, 8.6ms inference, 2.6ms NMS per image at shape (32, 3, 736, 736)\n","Results saved to \u001b[1m/content/drive/MyDrive/your_results_folder_name/my_yolo/exp5\u001b[0m\n"]}]},{"cell_type":"code","source":["## pruning 40%\n","!python val.py --weights /content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt  --data /content/drive/MyDrive/FaceDetectionDataset/data.yaml --img 720 --iou 0.65 --project /content/drive/MyDrive/your_results_folder_name/my_yolo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMMfoFXbeY_o","executionInfo":{"status":"ok","timestamp":1678182617194,"user_tz":-60,"elapsed":23014,"user":{"displayName":"Amany Waheeb","userId":"10921914126909105608"}},"outputId":"6389699c-a25b-4d32-fe65-797236d6a6d0"},"id":"LMMfoFXbeY_o","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=/content/drive/MyDrive/FaceDetectionDataset/data.yaml, weights=['/content/drive/MyDrive/your_results_folder_name/my_yolo/weights/best.pt'], batch_size=32, imgsz=720, conf_thres=0.001, iou_thres=0.65, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=/content/drive/MyDrive/your_results_folder_name/my_yolo, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 ğŸš€ v7.0-117-g85f6019 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n","WARNING âš ï¸ --img-size 720 must be multiple of max stride 32, updating to 736\n","Model pruned to 0.399 global sparsity\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/FaceDetectionDataset/valid/labels.cache... 336 images, 0 backgrounds, 0 corrupt: 100% 336/336 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11/11 [00:09<00:00,  1.17it/s]\n","                   all        336        361      0.851      0.778      0.876      0.526\n","                device        336        105      0.947       0.81      0.943      0.616\n","                  live        336         80      0.768      0.664      0.773      0.343\n","                  mask        336         79      0.989      0.823      0.982      0.693\n","                 photo        336         97        0.7      0.814      0.807      0.453\n","Speed: 1.5ms pre-process, 9.3ms inference, 2.0ms NMS per image at shape (32, 3, 736, 736)\n","Results saved to \u001b[1m/content/drive/MyDrive/your_results_folder_name/my_yolo/exp6\u001b[0m\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}